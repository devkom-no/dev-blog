---
title: "Cloud Providers and ipv4 usage"
date: 2022-09-29T21:50:53+02:00
draft: True
---
Some days ago, based on a question from a customer and as a way to "dust off" my python skills, i started writing a quick script/tool who fetches the Public IP information from the major cloud providers (AWS, GCP, Azure) and converts it to other formats ready to be pushed to network infrastructure (Firewalls, ACLs, LBs, Routers, BGP injection, etc). The final goal is to make the tool update the needed infrastructure with up-to-date information from the major cloud providers (for traffic engineering or just a FW rule for example).

While doing this, and inspired from a "100M AWS IPv4 addressess" researched by Andre Toonk that i read in another blog, i got inspired to check the current status of the Public IPv4 usage on the cloud providers, and compare their size based on the amount of addresses assigned to their services. (Just for fun, it shouldnt give a picture of the real size).

First we start with actually getting the data in. Where comes the data from? Well, GCP and AWS very kindly provide a quick way of fetching a .json file with a simpe GET that contains all their assigned IP ranges for both "Google" (Core services) and Google Cloud (GCP). The GCP data is also divided in regions but does not contain more info. For the purpose of this simple script, this is more than enough.

GOOGLE: https://www.gstatic.com/ipranges/goog.json
GCP: https://www.gstatic.com/ipranges/cloud.json

Amazon also provides this data the same way, available at:

AWS: https://ip-ranges.amazonaws.com/ip-ranges.json

Azure was a little more tricky, but lets get working on our python. In order to fetch this data programmatically and start playing with it i created a basic GET function using the requests module that does the HTTP get and returns the json response:

```python
def basicGET_JSON(url: str) -> dict:
    retries = 3
    for n in range(retries):
        try:
            response = requests.get(url)
            response.raise_for_status()
            break
        except HTTPError as exc:
            code = exc.response.status_code
            if code in [429, 500, 502, 503, 504]:
                # retry after n seconds
                time.sleep(n)
                continue
            raise
    return response.json()
```

So, with this function, I now define one to fetch GCP and one to fetch AWS. Two functions are not actually needed, but i want some modularity as the end goal is to apply some filtering and extra functionallity for the final tool. For the sake of brevity, lets just look the Amazon one, as the GCP/GOOG looks almost exactly the same:

```python
def fetchGCP(f=["all", "ipv4"]):
    GCP_URL = "https://www.gstatic.com/ipranges/cloud.json"
    GCP_JSON = basicGET_JSON(GCP_URL)
    if "ipv4" in f:
        # Going through the json and extracting all ipv4 prefixes into a list
        GCP_PREFIXES = [_.get("ipv4Prefix") for _ in GCP_JSON["prefixes"]]
        # Removing "None" results created by the ipv4-only query
        GCP_PREFIXES = [_ for _ in GCP_PREFIXES if _ != None]
    elif "ipv6" in f:
        # Going through the json and extracting all ipv6 prefixes into a list
        GCP_PREFIXES = [_.get("ipv6Prefix") for _ in GCP_JSON["prefixes"]]
        # Removing "None" results created by the ipv6-only query
        GCP_PREFIXES = [_ for _ in GCP_PREFIXES if _ != None]
    return GCP_PREFIXES
```

The Amazon one is actually easier as is divided on ipv4 and ipv6 prefixed, therefore i dont need to do any "cleaning" after filtering.

An example of the response that we get from Google is:

```python
>>> pprint.pprint(test)
['34.80.0.0/15',
 '34.137.0.0/16',
 '35.185.128.0/19',
 '35.185.160.0/20',
 '35.187.144.0/20',
 '35.189.160.0/19',
 '35.194.128.0/17',
 '35.201.128.0/17',
 '35.206.192.0/18',
 '35.220.32.0/21',
 '35.221.128.0/17',
 '35.229.128.0/17',
 '35.234.0.0/18',
...
```

Now into Azure. For some unknown reason, Microsoft does not provide a direct link into their .json file, but they provide a website with a button that contains the actual link, but the link is updated everytime the file changes. So, for Azure, i was forced to do first a GET towards the first link, parse the response looking for the .json file link, and then downloading the json.

After i have the json, i apply the same "filtering" as i did for Google or AWS:

```python
def fetchAzure(f=["all", "ipv4"]):
    AZURE_URL = "https://www.microsoft.com/en-us/download/confirmation.aspx?id=56519"
    AZURE_HTTP = requests.get(AZURE_URL)
    AZURE_JSON_URL = re.search(
        r"https:\/\/download\.\S*\.json", AZURE_HTTP.text
    ).group()
    AZURE_JSON = basicGET_JSON(AZURE_JSON_URL)
    TEMP_PREFIXES, AZURE_PREFIXES = [], []
    for _ in AZURE_JSON["values"]:
        TEMP_PREFIXES.extend([x for x in _["properties"]["addressPrefixes"]])
    for prefix in TEMP_PREFIXES:
        address = ipaddress.ip_network(prefix)
        if "ipv4" in f and isinstance(address, ipaddress.IPv4Network):
            AZURE_PREFIXES.append(prefix)
        elif "ipv6" in f and isinstance(address, ipaddress.IPv6Network):
            AZURE_PREFIXES.append(prefix)
    return AZURE_PREFIXES
```

Now, with the data, the rest is just math. Lets define some counters and fetch the data:

```python
aws_count, gcp_count, goog_count, azure_count = 0, 0, 0, 0

f = ["ipv4]
gcp_list = fetchGCP(f)
goog_list = fetchGOOG(f)
aws_list = fetchAWS(f)
azure_list = fetchAzure(f)
```

So now, i have the subnets in a list. But i want to know the total number of ip addresses on each subnet, so im going to use the ipaddress python module, specifically the num_address method

From the [documentation](https://docs.python.org/3/library/ipaddress.html)

```
ipaddress provides the capabilities to create, manipulate and operate on IPv4 and IPv6 addresses and networks."

Network objects

All attributes implemented by address objects are implemented by network objects as well. In addition, network objects implement additional attributes. All of these are common between IPv4Network and IPv6Network, so to avoid duplication they are only documented for IPv4Network. Network objects are hashable, so they can be used as keys in dictionaries.

class ipaddress.IPv4Network(address, strict=True)

    Construct an IPv4 network definition. address can be one of the following:

num_addresses

    The total number of addresses in the network.
```

So, with this information, i first define an ip_hosts function that transforms my v4 subnet string into a IPv4Network object:

```python
def ip_hosts(_):
    return ipaddress.IPv4Network(_)
```

Now we can loop through all the list, and update the counters with sum of all the addresses

```python
for _ in gcp_list:
    gcp_count += ip_hosts(_).num_addresses

for _ in goog_list:
    goog_count += ip_hosts(_).num_addresses

for _ in aws_list:
    aws_count += ip_hosts(_).num_addresses

for _ in azure_list:
    azure_count += ip_hosts(_).num_addresses
```

Now, with the counters, is just a matter of printing or doing some math with them:

```python
    print(f"Number of GCP addresses: {gcp_count:,}")
    print(f"Numer of GOOG addresses: {goog_count:,}")
    print(f"Total GOOG (Google backbone/services + GCP): {gcp_count + goog_count:,}")
    print(f"Total AWS addresses: {aws_count:,}")
    print(f"Total Azure addresses: {azure_count:,}")
    print(f"\n Total cloud: {gcp_count + goog_count + aws_count + azure_count:,}")
```

The final result is:

```
Number of GCP addresses: 11,402,496
Numer of GOOG addresses: 19,065,856
Total GOOG (Google backbone/services + GCP): 30,468,352
Total AWS addresses: 136,870,388
Total Azure addresses: 60,135,143

 Total cloud: 227,473,883
 ```

 This means, that based on IPv4 usage the sizing would be: AWS>Azure>GOOG. Which is a surprise for me, as i never imagined that Azure would be "bigger" than Google core + GCP combined! Maybe Google is more heavy on IPv6 ?

 Talking about IPv6, what are the ipv6 numbers? using the same script, and modifying the filtering, we get:

```
Number of GCP addresses: 716,893,011,031,475,100,600,762,368
Numer of GOOG addresses: 1,902,094,870,361,986,792,382,504,370,176
Total GOOG (Google backbone/services + GCP): 1,902,811,763,373,018,267,483,105,132,544
Total AWS addresses: 1,217,286,314,170,012,654,257,871,781,900
Total Azure addresses: 3,156,118,579,024,291,454,408,966,192

Total cloud: 3,123,254,196,122,055,213,195,385,880,636
```

Which then would be Google > AWS > Azure for ipv6!



A quick search of the total public ipv4 addresses gives the number "3,706,452,992". So, with some quick math, we could say that:

Total GOOG (Google backbone/services + GCP): 30,468,352 -> 0.8%
Total AWS addresses: 136,869,360 -> 3.7%
Total Azure addresses: 60,135,143 -> 1.7%

Total cloud: 227,472,855 -> 6.1%